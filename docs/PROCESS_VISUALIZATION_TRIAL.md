# \# Process Visualization â€” rag-ollama-set-up (end-to-end)

This page visualizes the full workflow we implemented:
- PostgreSQL + pgvector (vector DB + HNSW index)
- Ollama embeddings (bge-m3) + Ollama chat (gemma3:4b)
- Python scripts for ingestion, retrieval, generation, logging, and testing

---

```mermaid
flowchart TD

  subgraph Setup["Setup<br/>"]
    S1["Install + Configure"] --> S2["DB ready + Models ready"]
  end  

  S2 --> R0["â–¶ Start Runtime"]
  subgraph Runtime["Runtime"]
    R0 --> Ingest["ğŸ“¥ Ingest docs (ingest.py)"]
    Ingest --> Chunk["âœ‚ï¸ Chunk text"]
    Chunk --> EmbedDocs["ğŸ§  Embed docs (bge-m3)"]
    EmbedDocs --> Store["ğŸ—„ï¸ Store in rag_chunks"]
  end

```
---

```mermaid
flowchart TD

  START([ğŸš€ Start]) --> AUTH{"ğŸ” Authentication enabled?"}

  AUTH -- "No" --> ROLE["ğŸ‘¤ Treat as anonymous / default role"]
  AUTH -- "Yes" --> LOGIN["ğŸªª Authenticate user"]
  LOGIN --> OK{"âœ… Auth success?"}
  OK -- "No" --> DENY["âŒ Access denied"] --> END0([â›” End])
  OK -- "Yes" --> ROLE["ğŸ·ï¸ Load user role + groups"]

  ROLE --> PERM["ğŸ›¡ï¸ Permission check<br/>what data can this user access?"]
  PERM --> HASACCESS{Any access at all?}
  HASACCESS -- "No" --> DENY --> END0
  HASACCESS -- "Yes" --> ROUTE[RAG routing rules]

  ROUTE --> HASRAG{Is there a RAG for this question?}
  HASRAG -- "No" --> LLMONLY["Ask LLM directly (no RAG)"]
  HASRAG -- "Yes" --> QEMB["Embed question (bge-m3)"]
  

  Q --> S[ğŸ” Search the database for most similar saved parts]
  S --> R[ğŸ“¦ Pick the 5 most relevant text snippets and how strongly they match the question]

  R --> G{ğŸ¯ Relevant enough?}

  G -- "No ğŸ˜•" --> N[ğŸ™… Reply: Not enough info<br/>Ask user for more context]
  N --> L1[(ğŸ“ Log to qa_log)]
  classDef animate stroke-dasharray: 9,5,stroke-dashoffset: 900,animation: dash 25s linear infinite;
  class e1 animate

  G -- "Yes âœ…" --> C[ğŸ§© Build context prompt<br/>chunks + citations]
  C --> A[ğŸ’¬ Answer with Ollama<br/>gemma3:4b]
  A --> L2[(ğŸ“ Log to qa_log)]

  L2 --> AU{ğŸ•µï¸ Audit enabled?}
  AU -- "No" --> DONE([âœ… Done])

  AU -- "Yes" --> J[ğŸ§ª LLM Judge: quality + groundedness]
  J --> V{âœ… Pass?}

  V -- "Yes" --> DONE
  V -- "No" --> F[(ğŸ—³ï¸ Store feedback / failure)]
  F --> DONE

```

---

```mermaid

flowchart LR

  D["ğŸ“ Best similarity score"] --> T{"score â‰¤ MAX_DISTANCE?"}
  H["ğŸ”¢ Relevant chunks found"] --> M{"hits â‰¥ MIN_HITS?"}
  C["ğŸ§¾ Context length"] --> K{"chars â‰¥ MIN_CONTEXT_CHARS?"}

  T --> AND{"All checks pass?"}
  M --> AND
  K --> AND

  AND -- "Yes âœ…" --> G["âœ… Relevant â†’ Use RAG context"]
  AND -- "No âŒ" --> N["âŒ Not relevant â†’ No-RAG / ask for more info"]


```
