# \# Process Visualization â€” rag-ollama-set-up (end-to-end)

This page visualizes the full workflow we implemented:
- PostgreSQL + pgvector (vector DB + HNSW index)
- Ollama embeddings (bge-m3) + Ollama chat (gemma3:4b)
- Python scripts for ingestion, retrieval, generation, logging, and testing

---

```mermaid
flowchart TD

 subgraph Runtime["Runtime"]

  Ingest["ğŸ“¥ Ingest docs (ingest.py)"] --> Chunk["âœ‚ï¸ Chunk text"]
  Chunk --> EmbedDocs["ğŸ§  Embed docs (bge-m3)"]
  EmbedDocs --> Store["ğŸ—„ï¸ Store in rag_chunks"]

  U([ğŸ‘¤ User asks a question]) --> Q[ğŸ§  Embed question<br/>Ollama: bge-m3]

  Q --> S[ğŸ” Vector search in Postgres<br/>pgvector HNSW]
  S --> R[ğŸ“¦ Top-K chunks + distances]

  R --> G{ğŸ¯ Relevant enough?}

  G -- "No ğŸ˜•" --> N[ğŸ™… Reply: Not enough info<br/>Ask user for more context]
  N --> L1[(ğŸ“ Log to qa_log)]

  G -- "Yes âœ…" --> C[ğŸ§© Build context prompt<br/>chunks + citations]
  C --> A[ğŸ’¬ Answer with Ollama<br/>gemma3:4b]
  A --> L2[(ğŸ“ Log to qa_log)]

  L2 --> AU{ğŸ•µï¸ Audit enabled?}
  AU -- "No" --> DONE([âœ… Done])

  AU -- "Yes" --> J[ğŸ§ª LLM Judge: quality + groundedness]
  J --> V{âœ… Pass?}

  V -- "Yes" --> DONE
  V -- "No" --> F[(ğŸ—³ï¸ Store feedback / failure)]
  F --> DONE
```

---

```mermaid

flowchart LR
  D[ğŸ“ Best distance] --> T{<= RELEVANCE_MAX_DISTANCE?}
  H[ğŸ”¢ Relevant hits] --> M{>= MIN_RELEVANT_HITS?}
  C[ğŸ§¾ Context length] --> K{>= MIN_CONTEXT_CHARS?}

  T --> G[âœ… Gate = Relevant]
  M --> G
  K --> G



```